{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d613d9-afc9-47d0-94c3-fb900ce27338",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\nCollecting termcolor>=1.1.0\n  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (4.1.1)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\nCollecting flatbuffers>=23.1.21\n  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (21.3)\nCollecting astunparse>=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting grpcio<2.0,>=1.24.3\n  Using cached grpcio-1.56.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\nCollecting google-pasta>=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\nCollecting tensorflow-estimator<2.14,>=2.13.0\n  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\nCollecting opt-einsum>=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: six>=1.12.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\nCollecting tensorboard<2.14,>=2.13\n  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\nCollecting libclang>=13.0.0\n  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\nCollecting absl-py>=1.0.0\n  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\nCollecting wrapt>=1.11.0\n  Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\nCollecting keras<2.14,>=2.13.1\n  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\nCollecting gast<=0.4.0,>=0.2.1\n  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting numpy<=1.24.3,>=1.22\n  Using cached numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\nCollecting h5py>=2.9.0\n  Using cached h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\nCollecting google-auth-oauthlib<1.1,>=0.5\n  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nCollecting markdown>=2.6.8\n  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0\n  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\nCollecting werkzeug>=1.0.1\n  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.1)\nCollecting google-auth<3,>=1.6.3\n  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\nCollecting cachetools<6.0,>=2.0.0\n  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.9)\nCollecting pyasn1-modules>=0.2.1\n  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\nCollecting rsa<5,>=3.1.4\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib>=0.7.0\n  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata>=4.4\n  Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\nCollecting zipp>=0.5\n  Using cached zipp-3.16.2-py3-none-any.whl (7.2 kB)\nCollecting pyasn1<0.6.0,>=0.4.6\n  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2021.10.8)\nCollecting oauthlib>=3.0.0\n  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\nCollecting MarkupSafe>=2.1.1\n  Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, werkzeug, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.0.1\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-77ecf572-0afe-4e13-a9d8-512f8b57e0d1\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-77ecf572-0afe-4e13-a9d8-512f8b57e0d1\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-77ecf572-0afe-4e13-a9d8-512f8b57e0d1\n    Can't uninstall 'numpy'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.3 which is incompatible.\nSuccessfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 h5py-3.9.0 importlib-metadata-6.8.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.3 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0 zipp-3.16.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72120825-c9ff-4d89-9f74-f166d394d7fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "spark.conf.set(\"com.databricks.training.module-name\", \"deep-learning\")\n",
    "spark.conf.set(\"com.databricks.training.expected-dbr\", \"6.4\")\n",
    "\n",
    "spark.conf.set(\"com.databricks.training.suppress.untilStreamIsReady\", \"true\")\n",
    "spark.conf.set(\"com.databricks.training.suppress.stopAllStreams\", \"true\")\n",
    "spark.conf.set(\"com.databricks.training.suppress.moduleName\", \"true\")\n",
    "spark.conf.set(\"com.databricks.training.suppress.lessonName\", \"true\")\n",
    "# spark.conf.set(\"com.databricks.training.suppress.username\", \"true\")\n",
    "spark.conf.set(\"com.databricks.training.suppress.userhome\", \"true\")\n",
    "# spark.conf.set(\"com.databricks.training.suppress.workingDir\", \"true\")\n",
    "spark.conf.set(\"com.databricks.training.suppress.databaseName\", \"true\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "def display_run_uri(experiment_id, run_id):\n",
    "    host_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"browserHostName\").get()\n",
    "    uri = \"https://{}/#mlflow/experiments/{}/runs/{}\".format(host_name,experiment_id,run_id)\n",
    "    displayHTML(\"\"\"<b>Run URI:</b> <a href=\"{}\">{}</a>\"\"\".format(uri,uri))\n",
    "\n",
    "def waitForMLflow():\n",
    "  try:\n",
    "    import mlflow; \n",
    "    if int(mlflow.__version__.split(\".\")[1]) >= 2:\n",
    "        print(\"\"\"The module \"mlflow\" is attached and ready to go.\"\"\");\n",
    "    else:\n",
    "        print(\"\"\"You need MLflow version 1.2.0+ installed.\"\"\")\n",
    "  except ModuleNotFoundError:\n",
    "    print(\"\"\"The module \"mlflow\" is not yet attached to the cluster, waiting...\"\"\");\n",
    "    while True:\n",
    "      try: import mlflow; print(\"\"\"The module \"mlflow\" is attached and ready to go.\"\"\"); break;\n",
    "      except ModuleNotFoundError: import time; time.sleep(1); print(\".\", end=\"\");\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,f1_score,accuracy_score,fbeta_score,precision_score,recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "displayHTML(\"Preparing the learning environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "340383df-1b0a-4098-b91d-0f6998c4a937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Common-Notebooks/Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9bd4728-e025-4964-a2d2-5816124403c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "courseAdvertisements[\"waitForMLflow\"] =         (\"f\", \"\", \"MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility and deployment\")\n",
    "courseAdvertisements[\"display_run_uri\"] =       (\"f\", \"experiment_id, run_id\", \"Experiment and run ids\")\n",
    "courseAdvertisements[\"plot_confusion_matrix\"] = (\"f\", \"y_true, y_pred, classes, title=None, cmap=plt.cm.Blues\", \"Confusion matrix\")\n",
    "\n",
    "working_path = workingDir.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "courseAdvertisements[\"working_path\"] = (\"v\", working_path, \"This is working directory.\")\n",
    "# Make sure workingDir exists before continuing\n",
    "dbutils.fs.mkdirs(working_path.replace(\"/dbfs/\", \"dbfs:/\"))\n",
    "\n",
    "# Optimized fuse mount\n",
    "ml_working_path = f\"/dbfs/ml/{username.replace('+', '')}\"\n",
    "courseAdvertisements[\"ml_working_path\"] = (\"v\", ml_working_path, \"This is ML working directory.\")\n",
    "# Make sure workingDir exists before continuing\n",
    "dbutils.fs.mkdirs(ml_working_path.replace(\"/dbfs/\", \"dbfs:/\"))\n",
    "\n",
    "dbutils.fs.mkdirs(f\"{workingDir}/temp\")\n",
    "\n",
    "allDone(courseAdvertisements)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Classroom-Setup",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
