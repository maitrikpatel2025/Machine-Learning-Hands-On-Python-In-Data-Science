{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa7757f-a6c0-434b-9065-1d92069e5020",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLflow\n",
    "\n",
    "[MLflow](https://mlflow.org/docs/latest/concepts.html) seeks to address these three core issues:\n",
    "\n",
    "* It’s difficult to keep track of experiments\n",
    "* It’s difficult to reproduce code\n",
    "* There’s no standard way to package and deploy models\n",
    "\n",
    "In the past, when examining a problem, you would have to manually keep track of the many models you created, as well as their associated parameters and metrics. This can quickly become tedious and take up valuable time, which is where MLflow comes in.\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n",
    "* Use MLflow to track experiments, log metrics, and compare runs\n",
    "\n",
    "**Required Libraries**: \n",
    "* `mlflow==1.7.0` via PyPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ad9f57-7856-4c39-8216-2dbe665594dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-tracking.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b552035b-6670-4dce-8280-9e6318d6dbd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\nCollecting termcolor>=1.1.0\n  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (4.1.1)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\nCollecting flatbuffers>=23.1.21\n  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (21.3)\nCollecting astunparse>=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting grpcio<2.0,>=1.24.3\n  Using cached grpcio-1.56.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\nCollecting google-pasta>=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\nCollecting tensorflow-estimator<2.14,>=2.13.0\n  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\nCollecting opt-einsum>=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: six>=1.12.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\nCollecting tensorboard<2.14,>=2.13\n  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\nCollecting libclang>=13.0.0\n  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\nCollecting absl-py>=1.0.0\n  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\nCollecting wrapt>=1.11.0\n  Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\nCollecting keras<2.14,>=2.13.1\n  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\nCollecting gast<=0.4.0,>=0.2.1\n  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting numpy<=1.24.3,>=1.22\n  Using cached numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\nCollecting h5py>=2.9.0\n  Using cached h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\nCollecting google-auth-oauthlib<1.1,>=0.5\n  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nCollecting markdown>=2.6.8\n  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0\n  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\nCollecting werkzeug>=1.0.1\n  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.1)\nCollecting google-auth<3,>=1.6.3\n  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\nCollecting cachetools<6.0,>=2.0.0\n  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.9)\nCollecting pyasn1-modules>=0.2.1\n  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\nCollecting rsa<5,>=3.1.4\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib>=0.7.0\n  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata>=4.4\n  Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\nCollecting zipp>=0.5\n  Using cached zipp-3.16.2-py3-none-any.whl (7.2 kB)\nCollecting pyasn1<0.6.0,>=0.4.6\n  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2021.10.8)\nCollecting oauthlib>=3.0.0\n  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\nCollecting MarkupSafe>=2.1.1\n  Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, werkzeug, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.0.1\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe\n    Can't uninstall 'numpy'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.3 which is incompatible.\nSuccessfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 h5py-3.9.0 importlib-metadata-6.8.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.3 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0 zipp-3.16.2\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Preparing the learning environment..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Preparing the learning environment...",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Preparing the Scala environment..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Preparing the Scala environment...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Preparing the Scala environment..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Preparing the Scala environment...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Preparing the Python environment..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Preparing the Python environment...",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Preparing the Python environment..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Preparing the Python environment...",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Defining courseware-specific utility methods..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Defining courseware-specific utility methods...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Defining courseware-specific utility methods..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Defining courseware-specific utility methods...",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Defining user-facing utility methods..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Defining user-facing utility methods...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Defining custom variables for this lesson..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Defining custom variables for this lesson...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Defining custom variables for this lesson..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Defining custom variables for this lesson...",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initializing Databricks Academy's testing framework..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initializing Databricks Academy's testing framework...",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initializing Databricks Academy's testing framework..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initializing Databricks Academy's testing framework...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initializing Databricks Academy's services for generating dynamic data..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initializing Databricks Academy's services for generating dynamic data...",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initializing Databricks Academy's services for generating dynamic data..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initializing Databricks Academy's services for generating dynamic data...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Mounting course-specific datasets to <b>/mnt/training</b>...</br>Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtraineastus2.blob.core.windows.net/</b>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Mounting course-specific datasets to <b>/mnt/training</b>...</br>Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtraineastus2.blob.core.windows.net/</b>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Cleaning up the learning environment...no actions taken."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Cleaning up the learning environment...no actions taken.",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Cleaning up the learning environment...no actions taken."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Cleaning up the learning environment...no actions taken.",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n    ",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n    ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "The following functions were defined for you:<ul style='margin-top:0'><li style=\"cursor:help\" onclick=\"document.getElementById('waitForMLflow').style.display='block'\">\n",
       "        <span style=\"color: green; font-weight:bold\">waitForMLflow</span>\n",
       "        <span style=\"font-weight:bold\">(</span>\n",
       "        <span style=\"color: green; font-weight:bold; font-style:italic\"></span>\n",
       "        <span style=\"font-weight:bold\">)</span>\n",
       "        <div id=\"waitForMLflow\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility and deployment</div>\n",
       "        </li><li style=\"cursor:help\" onclick=\"document.getElementById('display_run_uri').style.display='block'\">\n",
       "        <span style=\"color: green; font-weight:bold\">display_run_uri</span>\n",
       "        <span style=\"font-weight:bold\">(</span>\n",
       "        <span style=\"color: green; font-weight:bold; font-style:italic\">experiment_id, run_id</span>\n",
       "        <span style=\"font-weight:bold\">)</span>\n",
       "        <div id=\"display_run_uri\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">Experiment and run ids</div>\n",
       "        </li><li style=\"cursor:help\" onclick=\"document.getElementById('plot_confusion_matrix').style.display='block'\">\n",
       "        <span style=\"color: green; font-weight:bold\">plot_confusion_matrix</span>\n",
       "        <span style=\"font-weight:bold\">(</span>\n",
       "        <span style=\"color: green; font-weight:bold; font-style:italic\">y_true, y_pred, classes, title=None, cmap=plt.cm.Blues</span>\n",
       "        <span style=\"font-weight:bold\">)</span>\n",
       "        <div id=\"plot_confusion_matrix\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">Confusion matrix</div>\n",
       "        </li></ul>The following variables were defined for you:<ul style='margin-top:0'><li style=\"cursor:help\" onclick=\"document.getElementById('username').style.display='block'\">\n",
       "        <span style=\"color: green; font-weight:bold\">username</span>: <span style=\"font-style:italic; font-weight:bold\">maitrik.patel2025@gmail.com </span>\n",
       "        <div id=\"username\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">No additional information was provided.</div>\n",
       "        </li><li style=\"cursor:help\" onclick=\"document.getElementById('workingDir').style.display='block'\">\n",
       "        <span style=\"color: green; font-weight:bold\">workingDir</span>: <span style=\"font-style:italic; font-weight:bold\">dbfs:/user/maitrik.patel2025@gmail.com/deep_learning/1_mlflow_pil </span>\n",
       "        <div id=\"workingDir\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">No additional information was provided.</div>\n",
       "        </li><li style=\"cursor:help\" onclick=\"document.getElementById('working_path').style.display='block'\">\n",
       "        <span style=\"color: green; font-weight:bold\">working_path</span>: <span style=\"font-style:italic; font-weight:bold\">/dbfs/user/maitrik.patel2025@gmail.com/deep_learning/1_mlflow_pil </span>\n",
       "        <div id=\"working_path\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">This is working directory.</div>\n",
       "        </li><li style=\"cursor:help\" onclick=\"document.getElementById('ml_working_path').style.display='block'\">\n",
       "        <span style=\"color: green; font-weight:bold\">ml_working_path</span>: <span style=\"font-style:italic; font-weight:bold\">/dbfs/ml/maitrik.patel2025@gmail.com </span>\n",
       "        <div id=\"ml_working_path\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">This is ML working directory.</div>\n",
       "        </li></ul>All done!"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "The following functions were defined for you:<ul style='margin-top:0'><li style=\"cursor:help\" onclick=\"document.getElementById('waitForMLflow').style.display='block'\">\n        <span style=\"color: green; font-weight:bold\">waitForMLflow</span>\n        <span style=\"font-weight:bold\">(</span>\n        <span style=\"color: green; font-weight:bold; font-style:italic\"></span>\n        <span style=\"font-weight:bold\">)</span>\n        <div id=\"waitForMLflow\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility and deployment</div>\n        </li><li style=\"cursor:help\" onclick=\"document.getElementById('display_run_uri').style.display='block'\">\n        <span style=\"color: green; font-weight:bold\">display_run_uri</span>\n        <span style=\"font-weight:bold\">(</span>\n        <span style=\"color: green; font-weight:bold; font-style:italic\">experiment_id, run_id</span>\n        <span style=\"font-weight:bold\">)</span>\n        <div id=\"display_run_uri\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">Experiment and run ids</div>\n        </li><li style=\"cursor:help\" onclick=\"document.getElementById('plot_confusion_matrix').style.display='block'\">\n        <span style=\"color: green; font-weight:bold\">plot_confusion_matrix</span>\n        <span style=\"font-weight:bold\">(</span>\n        <span style=\"color: green; font-weight:bold; font-style:italic\">y_true, y_pred, classes, title=None, cmap=plt.cm.Blues</span>\n        <span style=\"font-weight:bold\">)</span>\n        <div id=\"plot_confusion_matrix\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">Confusion matrix</div>\n        </li></ul>The following variables were defined for you:<ul style='margin-top:0'><li style=\"cursor:help\" onclick=\"document.getElementById('username').style.display='block'\">\n        <span style=\"color: green; font-weight:bold\">username</span>: <span style=\"font-style:italic; font-weight:bold\">maitrik.patel2025@gmail.com </span>\n        <div id=\"username\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">No additional information was provided.</div>\n        </li><li style=\"cursor:help\" onclick=\"document.getElementById('workingDir').style.display='block'\">\n        <span style=\"color: green; font-weight:bold\">workingDir</span>: <span style=\"font-style:italic; font-weight:bold\">dbfs:/user/maitrik.patel2025@gmail.com/deep_learning/1_mlflow_pil </span>\n        <div id=\"workingDir\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">No additional information was provided.</div>\n        </li><li style=\"cursor:help\" onclick=\"document.getElementById('working_path').style.display='block'\">\n        <span style=\"color: green; font-weight:bold\">working_path</span>: <span style=\"font-style:italic; font-weight:bold\">/dbfs/user/maitrik.patel2025@gmail.com/deep_learning/1_mlflow_pil </span>\n        <div id=\"working_path\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">This is working directory.</div>\n        </li><li style=\"cursor:help\" onclick=\"document.getElementById('ml_working_path').style.display='block'\">\n        <span style=\"color: green; font-weight:bold\">ml_working_path</span>: <span style=\"font-style:italic; font-weight:bold\">/dbfs/ml/maitrik.patel2025@gmail.com </span>\n        <div id=\"ml_working_path\" style=\"display:none; margin:0.5em 0; border-left: 3px solid grey; padding-left: 0.5em\">This is ML working directory.</div>\n        </li></ul>All done!",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69185f77-983d-4114-acf2-529b2060f7c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's start by loading SF Airbnb Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbe240b-231a-4f04-b854-61ebaf567cf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2177866436138535>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m filePath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/user/hive/warehouse/airbnb_listings\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m----> 2\u001B[0m airbnbDF \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(filePath)\n",
       "\u001B[1;32m      4\u001B[0m (trainDF, testDF) \u001B[38;5;241m=\u001B[39m airbnbDF\u001B[38;5;241m.\u001B[39mrandomSplit([\u001B[38;5;241m.8\u001B[39m, \u001B[38;5;241m.2\u001B[39m], seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(trainDF\u001B[38;5;241m.\u001B[39mcache()\u001B[38;5;241m.\u001B[39mcount())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:533\u001B[0m, in \u001B[0;36mDataFrameReader.parquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n",
       "\u001B[1;32m    522\u001B[0m int96RebaseMode \u001B[38;5;241m=\u001B[39m options\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint96RebaseMode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[1;32m    523\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m    524\u001B[0m     mergeSchema\u001B[38;5;241m=\u001B[39mmergeSchema,\n",
       "\u001B[1;32m    525\u001B[0m     pathGlobFilter\u001B[38;5;241m=\u001B[39mpathGlobFilter,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    530\u001B[0m     int96RebaseMode\u001B[38;5;241m=\u001B[39mint96RebaseMode,\n",
       "\u001B[1;32m    531\u001B[0m )\n",
       "\u001B[0;32m--> 533\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Incompatible format detected.\n",
       "\n",
       "A transaction log for Delta was found at `dbfs:/user/hive/warehouse/airbnb_listings/_delta_log`,\n",
       "but you are trying to read from `dbfs:/user/hive/warehouse/airbnb_listings` using format(\"parquet\"). You must use\n",
       "'format(\"delta\")' when reading and writing to a delta table.\n",
       "\n",
       "To disable this check, SET spark.databricks.delta.formatCheck.enabled=false\n",
       "To learn more about Delta, see https://docs.microsoft.com/azure/databricks/delta/index"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2177866436138535>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m filePath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/user/hive/warehouse/airbnb_listings\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m airbnbDF \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(filePath)\n\u001B[1;32m      4\u001B[0m (trainDF, testDF) \u001B[38;5;241m=\u001B[39m airbnbDF\u001B[38;5;241m.\u001B[39mrandomSplit([\u001B[38;5;241m.8\u001B[39m, \u001B[38;5;241m.2\u001B[39m], seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(trainDF\u001B[38;5;241m.\u001B[39mcache()\u001B[38;5;241m.\u001B[39mcount())\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:533\u001B[0m, in \u001B[0;36mDataFrameReader.parquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n\u001B[1;32m    522\u001B[0m int96RebaseMode \u001B[38;5;241m=\u001B[39m options\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint96RebaseMode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m    524\u001B[0m     mergeSchema\u001B[38;5;241m=\u001B[39mmergeSchema,\n\u001B[1;32m    525\u001B[0m     pathGlobFilter\u001B[38;5;241m=\u001B[39mpathGlobFilter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    530\u001B[0m     int96RebaseMode\u001B[38;5;241m=\u001B[39mint96RebaseMode,\n\u001B[1;32m    531\u001B[0m )\n\u001B[0;32m--> 533\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Incompatible format detected.\n\nA transaction log for Delta was found at `dbfs:/user/hive/warehouse/airbnb_listings/_delta_log`,\nbut you are trying to read from `dbfs:/user/hive/warehouse/airbnb_listings` using format(\"parquet\"). You must use\n'format(\"delta\")' when reading and writing to a delta table.\n\nTo disable this check, SET spark.databricks.delta.formatCheck.enabled=false\nTo learn more about Delta, see https://docs.microsoft.com/azure/databricks/delta/index",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Incompatible format detected.\n\nA transaction log for Delta was found at `dbfs:/user/hive/warehouse/airbnb_listings/_delta_log`,\nbut you are trying to read from `dbfs:/user/hive/warehouse/airbnb_listings` using format(\"parquet\"). You must use\n'format(\"delta\")' when reading and writing to a delta table.\n\nTo disable this check, SET spark.databricks.delta.formatCheck.enabled=false\nTo learn more about Delta, see https://docs.microsoft.com/azure/databricks/delta/index",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "filePath = \"dbfs:/mnt/training/airbnb/sf-listings/sf-listings-2019-03-06-clean.parquet/\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "\n",
    "(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "print(trainDF.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade0b6e0-2ce9-44e2-9276-105bcf5881e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MLflow Tracking\n",
    "\n",
    "MLflow Tracking is a logging API specific for machine learning and agnostic to libraries and environments that do the training.  It is organized around the concept of **runs**, which are executions of data science code.  Runs are aggregated into **experiments** where many runs can be a part of a given experiment and an MLflow server can host many experiments.\n",
    "\n",
    "\n",
    "MLflow tracking also serves as a **model registry** so tracked models can easily be stored and, as necessary, deployed into production. This also standardizes this process, which significantly accelerates it and allows for scalability. Experiments can be tracked using libraries in Python, R, and Java as well as by using the CLI and REST calls.  This module will use Python, though the majority of MLflow functionality is also exposed in these other APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6147f125-1fa1-4928-b9f9-378d01f107de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Track Runs\n",
    "\n",
    "Each run can record the following information:<br><br>\n",
    "\n",
    "- **Parameters:** Key-value pairs of input parameters such as the number of trees in a random forest model\n",
    "- **Metrics:** Evaluation metrics such as RMSE or Area Under the ROC Curve\n",
    "- **Artifacts:** Arbitrary output files in any format.  This can include images, pickled models, and data files\n",
    "- **Source:** The code that originally ran the experiment\n",
    "\n",
    "**NOTE**: MLflow can only log PipelineModels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f63ca24-9628-4345-bda8-213c4e3f2931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting mlflow\n  Downloading mlflow-2.5.0-py3-none-any.whl (18.2 MB)\nCollecting gunicorn<21\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\nCollecting docker<7,>=4.0.0\n  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2021.3)\nCollecting pyyaml<7,>=5.1\n  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\nCollecting gitpython<4,>=2.1.0\n  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\nCollecting cloudpickle<3\n  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe/lib/python3.9/site-packages (from mlflow) (4.23.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.27.1)\nRequirement already satisfied: numpy<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe/lib/python3.9/site-packages (from mlflow) (1.24.3)\nCollecting alembic!=1.10.0,<2\n  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\nCollecting Flask<3\n  Downloading Flask-2.3.2-py3-none-any.whl (96 kB)\nRequirement already satisfied: pyarrow<13,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4)\nRequirement already satisfied: markdown<4,>=3.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe/lib/python3.9/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\nCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe/lib/python3.9/site-packages (from mlflow) (6.8.0)\nCollecting sqlalchemy<3,>=1.4.0\n  Downloading SQLAlchemy-2.0.19-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\nRequirement already satisfied: packaging<24 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (21.3)\nCollecting sqlparse<1,>=0.4.0\n  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\nCollecting databricks-cli<1,>=0.8.7\n  Downloading databricks-cli-0.17.7.tar.gz (83 kB)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (8.0.4)\nCollecting Mako\n  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\nCollecting pyjwt>=1.7.0\n  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\nRequirement already satisfied: oauthlib>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\nCollecting tabulate>=0.7.7\n  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\nRequirement already satisfied: urllib3<2.0.0,>=1.26.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.9)\nCollecting websocket-client>=0.32.0\n  Downloading websocket_client-1.6.1-py3-none-any.whl (56 kB)\nRequirement already satisfied: Werkzeug>=2.3.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe/lib/python3.9/site-packages (from Flask<3->mlflow) (2.3.6)\nCollecting click<9,>=7.0\n  Downloading click-8.1.5-py3-none-any.whl (98 kB)\nCollecting itsdangerous>=2.1.2\n  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\nCollecting blinker>=1.6.2\n  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\nCollecting Jinja2<4,>=2.11\n  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.9/site-packages (from gunicorn<21->mlflow) (61.2.0)\nRequirement already satisfied: zipp>=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.16.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\nCollecting numpy<2\n  Downloading numpy-1.22.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\nCollecting typing-extensions>=4\n  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\nBuilding wheels for collected packages: databricks-cli\n  Building wheel for databricks-cli (setup.py): started\n  Building wheel for databricks-cli (setup.py): finished with status 'done'\n  Created wheel for databricks-cli: filename=databricks_cli-0.17.7-py3-none-any.whl size=143878 sha256=d214ad5f08a22f45046217d7b80068d9362ba8c4e1a0976cdcf3b7acf49d3f0a\n  Stored in directory: /root/.cache/pip/wheels/b6/90/68/94d223a35a3910c1512a8d42d9f8333ce567ef26e250a56227\nSuccessfully built databricks-cli\nInstalling collected packages: typing-extensions, smmap, numpy, greenlet, websocket-client, tabulate, sqlalchemy, pyjwt, Mako, Jinja2, itsdangerous, gitdb, click, blinker, sqlparse, querystring-parser, pyyaml, gunicorn, gitpython, Flask, docker, databricks-cli, cloudpickle, alembic, mlflow\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.3\n    Uninstalling numpy-1.24.3:\n      Successfully uninstalled numpy-1.24.3\n  Attempting uninstall: Jinja2\n    Found existing installation: Jinja2 2.11.3\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b91705da-0ead-462f-a571-cc2cae942cfe\n    Can't uninstall 'click'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\nSuccessfully installed Flask-2.3.2 Jinja2-3.1.2 Mako-1.2.4 alembic-1.11.1 blinker-1.6.2 click-8.1.5 cloudpickle-2.2.1 databricks-cli-0.17.7 docker-6.1.3 gitdb-4.0.10 gitpython-3.1.32 greenlet-2.0.2 gunicorn-20.1.0 itsdangerous-2.1.2 mlflow-2.5.0 numpy-1.22.4 pyjwt-2.7.0 pyyaml-6.0 querystring-parser-1.2.4 smmap-5.0.0 sqlalchemy-2.0.19 sqlparse-0.4.4 tabulate-0.9.0 typing-extensions-4.7.1 websocket-client-1.6.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4c88e7-b9be-448e-b389-e5d0058ffdad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2177866436138538>:15\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m lr \u001B[38;5;241m=\u001B[39m LinearRegression(featuresCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     14\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline(stages\u001B[38;5;241m=\u001B[39m[vecAssembler, lr])\n",
       "\u001B[0;32m---> 15\u001B[0m pipelineModel \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mfit(trainDF)\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Log parameters\u001B[39;00m\n",
       "\u001B[1;32m     18\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice-bedrooms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    210\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:134\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    132\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n",
       "\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n",
       "\u001B[0;32m--> 134\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mstage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    135\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n",
       "\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    210\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:383\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n",
       "\u001B[0;32m--> 383\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    384\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n",
       "\u001B[1;32m    385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:380\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    377\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    379\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n",
       "\u001B[0;32m--> 380\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column price must be of type numeric but was actually of type string."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-2177866436138538>:15\u001B[0m\n\u001B[1;32m     13\u001B[0m lr \u001B[38;5;241m=\u001B[39m LinearRegression(featuresCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline(stages\u001B[38;5;241m=\u001B[39m[vecAssembler, lr])\n\u001B[0;32m---> 15\u001B[0m pipelineModel \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mfit(trainDF)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Log parameters\u001B[39;00m\n\u001B[1;32m     18\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mlog_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice-bedrooms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:134\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    132\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n\u001B[0;32m--> 134\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mstage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:383\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 383\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:380\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 380\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column price must be of type numeric but was actually of type string.",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: Column price must be of type numeric but was actually of type string.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "username = \"maitrik.patel2025@gmail.com\"\n",
    "mlflow.set_experiment(f\"/Users/{username}/tr-mlflow\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"LR-Single-Feature\") as run:\n",
    "  # Define pipeline\n",
    "  vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "  lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "  pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "  pipelineModel = pipeline.fit(trainDF)\n",
    "  \n",
    "  # Log parameters\n",
    "  mlflow.log_param(\"label\", \"price-bedrooms\")\n",
    "  \n",
    "  # Log model\n",
    "  mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "  \n",
    "  # Evaluate predictions\n",
    "  predDF = pipelineModel.transform(testDF)\n",
    "  regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "  rmse = regressionEvaluator.evaluate(predDF)\n",
    "  \n",
    "  # Log metrics\n",
    "  mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "# display_run_uri(run.info.experiment_id, run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0792342-0e3f-4c41-bb5f-24dbf29af0a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next let's build our linear regression model but use all of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55085005-7a05-4486-80fe-1d10c2481f79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.feature import RFormula\n",
    "with mlflow.start_run(run_name=\"LR-All-Features\") as run:\n",
    "  # Create pipeline\n",
    "  rFormula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")\n",
    "  lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "  pipeline = Pipeline(stages = [rFormula, lr])\n",
    "  pipelineModel = pipeline.fit(trainDF)\n",
    "  \n",
    "  # Log pipeline\n",
    "  mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "  \n",
    "  # Log parameter\n",
    "  mlflow.log_param(\"label\", \"price-all-features\")\n",
    "  \n",
    "  # Create predictions and metrics\n",
    "  predDF = pipelineModel.transform(testDF)\n",
    "  regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "  rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "  r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "  \n",
    "  # Log both metrics\n",
    "  mlflow.log_metric(\"rmse\", rmse)\n",
    "  mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "# display_run_uri(run.info.experiment_id, run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c41ad7-5678-4fc8-bba7-4e111315372a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, we will use Linear Regression to predict the log of the price, due to its log normal distribution.\n",
    "\n",
    "We'll also practice logging artifacts to keep a visual of our log normal histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1848b143-f7ac-4188-a082-f8b7dacdc9b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.sql.functions import col, log, exp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with mlflow.start_run(run_name=\"LR-Log-Price\") as run:\n",
    "  # Take log of price\n",
    "  logTrainDF = trainDF.withColumn(\"log_price\", log(col(\"price\")))\n",
    "  logTestDF = testDF.withColumn(\"log_price\", log(col(\"price\")))\n",
    "  \n",
    "  # Log parameter\n",
    "  mlflow.log_param(\"label\", \"log-price\")\n",
    "  \n",
    "  # Create pipeline\n",
    "  rFormula = RFormula(formula=\"log_price ~ . - price\", featuresCol=\"features\", labelCol=\"log_price\", handleInvalid=\"skip\")  \n",
    "  lr = LinearRegression(labelCol=\"log_price\", predictionCol=\"log_prediction\")\n",
    "  pipeline = Pipeline(stages = [rFormula, lr])\n",
    "  pipelineModel = pipeline.fit(logTrainDF)\n",
    "  \n",
    "  # Log model\n",
    "  mlflow.spark.log_model(pipelineModel, \"log-model\")\n",
    "  \n",
    "  # Make predictions\n",
    "  predDF = pipelineModel.transform(logTestDF)\n",
    "  expDF = predDF.withColumn(\"prediction\", exp(col(\"log_prediction\")))\n",
    "  \n",
    "  # Evaluate predictions\n",
    "  rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(expDF)\n",
    "  r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(expDF)\n",
    "  \n",
    "  # Log metrics\n",
    "  mlflow.log_metric(\"rmse\", rmse)\n",
    "  mlflow.log_metric(\"r2\", r2)\n",
    "  \n",
    "  # Log artifact\n",
    "  plt.clf()\n",
    "  logTrainDF.toPandas().hist(column=\"log_price\", bins=100)\n",
    "  figPath = username + \"logNormal.png\" \n",
    "  plt.savefig(figPath)\n",
    "  mlflow.log_artifact(figPath)\n",
    "  display(plt.show())\n",
    "  \n",
    "# display_run_uri(run.info.experiment_id, run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5012410-3047-428a-bbc2-c1385043d798",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That's it! Now, let's use MLflow to easily look over our work and compare model performance. You can either query past runs programmatically or use the MLflow UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a688fc82-1795-4fe0-951d-1131ce46c2be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Querying Past Runs\n",
    "\n",
    "You can query past runs programatically in order to use this data back in Python.  The pathway to doing this is an `MlflowClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a0a4349-23e3-4ce4-bf55-f198102837a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c62246-22a0-4779-a845-438ba56e5f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8fd3c55-502e-4230-916c-f0c2aaa93374",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can also use [search_runs](https://mlflow.org/docs/latest/search-syntax.html) to find all runs for a given experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbcbbf2-45c4-4431-8403-697718fe45bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "experiment_id = run.info.experiment_id\n",
    "runs_df = mlflow.search_runs(experiment_id)\n",
    "\n",
    "display(runs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75ec5aa3-dff3-447a-94b2-c5914aa38cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pull the last run and look at metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113a16dc-6595-4ba1-a4e0-8155b7d0f2ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "runs = client.search_runs(experiment_id, order_by=[\"attributes.start_time desc\"], max_results=1)\n",
    "runs[0].data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdcf0e49-b3ce-4290-9856-f19d1f9ddaec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "run_id = runs[0].info.run_id\n",
    "# display_run_uri(run.info.experiment_id, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a46b4bfa-4109-43c9-920f-51d118987b9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "Examine the results in the UI.  Look for the following:<br><br>\n",
    "\n",
    "1. The `Experiment ID`\n",
    "2. The artifact location.  This is where the artifacts are stored in DBFS.\n",
    "3. The time the run was executed.  **Click this to see more information on the run.**\n",
    "4. The code that executed the run.\n",
    "\n",
    "\n",
    "After clicking on the time of the run, take a look at the following:<br><br>\n",
    "\n",
    "1. The Run ID will match what we printed above\n",
    "2. The model that we saved, included a pickled version of the model as well as the Conda environment and the `MLmodel` file.\n",
    "\n",
    "Note that you can add notes under the \"Notes\" tab to help keep track of important information about your models. \n",
    "\n",
    "Also, click on the run for the log normal distribution and see that the histogram is saved in \"Artifacts\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04955131-6929-41a3-a759-9c15a478dc4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Saved Model\n",
    "\n",
    "Let's practice [loading](https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html) our logged log-normal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d06bd840-cdf9-4f95-ba2e-b6e82281d68c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "loaded_model = mlflow.spark.load_model(f\"runs:/{run.info.run_uuid}/log-model\")\n",
    "display(loaded_model.transform(testDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1e4dce1-ee39-4cc4-b427-ebc9436a7125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Log Param, Metrics, and Artifacts\n",
    "\n",
    "Now it's your turn! Log your name, your height, and a fun [matplotlib visualization](https://matplotlib.org/3.1.0/gallery/lines_bars_and_markers/scatter_with_legend.html#sphx-glr-gallery-lines-bars-and-markers-scatter-with-legend-py) (by calling the `generate_plot` function below - feel free to modify the viz!) under a run with name `MLflow-Lab` in our new MLflow experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2685ea67-b37c-4e42-9108-530f9a122b72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def generate_plot():\n",
    "  import numpy as np\n",
    "  np.random.seed(19680801)\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  for color in ['tab:blue', 'tab:orange', 'tab:green']:\n",
    "      n = 750\n",
    "      x, y = np.random.rand(2, n)\n",
    "      scale = 200.0 * np.random.rand(n)\n",
    "      ax.scatter(x, y, c=color, s=scale, label=color,\n",
    "                 alpha=0.3, edgecolors='none')\n",
    "\n",
    "  ax.legend()\n",
    "  ax.grid(True)\n",
    "#   display(plt.show())\n",
    "  return fig, plt\n",
    "\n",
    "generate_plot()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1. MLflow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
